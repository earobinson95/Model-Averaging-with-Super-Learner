---
title: "Improved Classification with Ensemble Learning"
author: "Emily Robinson"
date: "November 12, 2018"
Course: "Statistical Methods III (Stat 823)"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("C:/Users/EmilyARobinson/Dropbox/823- Methods III/Project")

library(dplyr)
library(caret)
library(ggplot2)
library(car)
library(corrplot)
library(ROCR)

panel.cor <- function(x, y, digits=2, prefix="", cex.cor) 
{
  usr <- par("usr"); on.exit(par(usr)) 
  par(usr = c(0, 1, 0, 1)) 
  r <- abs(cor(x, y)) 
  txt <- format(c(r, 0.123456789), digits=digits)[1] 
  txt <- paste(prefix, txt, sep="") 
  if(missing(cex.cor)) cex <- 0.8/strwidth(txt) 
  
  test <- cor.test(x,y) 
  # borrowed from printCoefmat
  Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
                   cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                   symbols = c("***", "**", "*", ".", " ")) 
  
  text(0.5, 0.5, txt, cex = cex * r) 
  text(.8, .8, Signif, cex=cex, col=2) 
}

misclass = function(yhat,y) {
  temp <- table(yhat,y)
  cat("Table of Misclassification\n")
  cat("(row = predicted, col = actual)\n")
  print(temp)
  cat("\n\n")
  numcor <- sum(diag(temp))
  numinc <- length(y) - numcor
  mcr <- numinc/length(y)
  cr  <- sum(diag(temp))/sum(temp)
  cat(paste("Classification Rate = ",format(cr,digits=3)))
  cat("\n")
  cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
  cat("\n")
}

review_weights = function(cv_sl) {
  meta_weights = coef(cv_sl)
  means = colMeans(meta_weights)
  sds = apply(meta_weights, MARGIN = 2,  FUN = function(col) { sd(col) })
  mins = apply(meta_weights, MARGIN = 2, FUN = function(col) { min(col) })
  maxs = apply(meta_weights, MARGIN = 2, FUN = function(col) { max(col) })
  # Combine the stats into a single matrix.
  sl_stats = cbind("mean(weight)" = means, "sd" = sds, "min" = mins, "max" = maxs)
  # Sort by decreasing mean weight.
  sl_stats[order(sl_stats[, 1], decreasing = T), ]
}

ROC_eval <- function(fit, pred, y){
  pred_rocr <- ROCR::prediction(pred, Y_holdout)
  auc       <- ROCR::performance(pred_rocr, measure = "auc", x.measure = "cutoff")@y.values[[1]]
  perf      <- performance( pred_rocr, "tpr", "fpr" )
  plot(perf, main = paste("Fit =", fit, "\n", "AUC = ", round(auc,4)))
  abline(0,1)
}
```

####Overview
Statistical learning has become a thing of the new age. There are multiple choices of models for classification from logistic regression, support vector machines, discriminant analysis, and classification trees. The issue arises when deciding which one to use. A solution that may improve the classification rates is to combine all predictions into one, more improved prediction. [@Steinki2015]

####Ensemble Modeling

####Stacking
Consider $M=2$, then $E[y-\hat{f}_1(x)]=0$ and $E[y-\hat{f}_2(x)]=0$. Define $\hat{f}_\alpha = \alpha\hat{f}_1+(1-\alpha)\hat{f}_2. Then,

\begin{align}
E[(y-\hat{f}_\alpha(x))^2 &= E[(\alpha\hat{f}_1+(1-\alpha)\hat{f}_2)^2]]
& = 
\end{align}

####Diabetes Dataset
The dataset I have selected for my project is the Pima Indians Diabetes Database found on Kaggle through the UCI data repository, originally from the National Institute of Diabetes and Digestive and idney Diseases. The patients in this study are all females at least age 21 years old and of Pima Indian heritage. The target variable, Outcome, is binary and indicates whether or not a patient has diabetes. The objective is to correctly classify individuals to have diabetes or not based on several medical predictor variables such as the number of pregnancies the patient has had, their BMI, insulin level, age, etc. [@SmithJ1988Using]

```{r data, echo = FALSE}
setwd("C:/Users/EmilyARobinson/Dropbox/823- Methods III/Project")
Diabetes <- read.csv("diabetes.csv")
names(Diabetes)
```

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
glm_mod <- glm(Outcome ~., data = Diabetes, family = binomial())
outlierTest(glm_mod)
cutoff <- 4/((nrow(Diabetes)-length(glm_mod$coefficients)-2)) 
plot(glm_mod, which=4, cook.levels=cutoff)
influencePlot(glm_mod,	id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
pairs(Diabetes, lower.panel=panel.smooth, upper.panel=panel.cor)
```

### Data Preparation
```{r, dataPrep}
# Create response and predictor sets
Diabetes <- Diabetes[-c(14, 229, 350, 454, 503, 707),]
outcome  <- Diabetes$Outcome
data     <- subset(Diabetes, select = -Outcome)

# Create train & test sets
set.seed(904)
pct = 0.6
train_obs = sample(nrow(data), floor(nrow(data)*pct))
X_train = data[train_obs, ]
X_holdout = data[-train_obs, ]
Y_train = outcome[train_obs]
Y_holdout = outcome[-train_obs]

table(Y_train, useNA = "ifany")
```

### Explore Dataset
```{r, Explore}
glm_mod2 <- glm(Outcome ~., data = Diabetes, family = binomial())
glm_mod2$coefficients
```

### Variable Importance
```{r, VarImp}
library(randomForest)
rf_mod <- randomForest(factor(Outcome) ~ ., data = Diabetes, family = "binomial")
varImpPlot(rf_mod, main = "Variable Importance")
avPlots(glm_mod2)
```

#### ROC


####Super Learner

```{r, SuperLearner1}
library(SuperLearner)
listWrappers()
```

[@Naimi2018; @Polley2018; @Vardeman2018]

####Fit base models

```{r, fitGLM}
sl_glm = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = "SL.glm")
sl_glm
pred_glm = predict(sl_glm, X_holdout)
ROC_eval("glm", pred_glm$pred, Y_holdout)
```

```{r, codeGlmnet}
SL.glmnet
```

```{r, fitLasso}
sl_lasso = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = "SL.glmnet")
pred_lasso = predict(sl_lasso, X_holdout)
ROC_eval("lasso", pred_lasso$pred, Y_holdout)
```

```{r, fitRidge}
set.seed(904)
learners    = create.Learner("SL.glmnet", params = list(alpha = 0))
learners$names
sl_ridge = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = learners$names)
pred_ridge = predict(sl_ridge, X_holdout)
ROC_eval("ridge", pred_ridge$pred, Y_holdout)
```

```{r, fitRanger}
library(ranger)
set.seed(904)
sl_ranger = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = "SL.ranger")
pred_ranger = predict(sl_ranger, X_holdout)
ROC_eval("ranger", pred_ranger$pred, Y_holdout)
```


####Stacked Model
```{r, sl_stacked}
SL.library <- c("SL.glm", "SL.gam", "SL.glmnet", "SL.glmnet_1", "SL.ranger", "SL.ksvm", "SL.mean")
set.seed(904)
sl_stacked = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = SL.library, 
                          verbose = F, cvControl = list(V = 5), method = "method.NNLS")
sl_stacked
```


```{r, cv_stacked}
set.seed(904)
cv_stacked <- CV.SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = SL.library,
                              verbose = F, cvControl = list(V = 5), method = "method.NNLS")
review_weights(cv_stacked)
summary(cv_stacked)
plot(cv_stacked) + theme_bw(base_size = 15)
```


```{r, pred.eval, echo = F}
pred_sl <- predict.SuperLearner(sl_stacked, newdata = X_holdout, Y = Y_train, X = X_train)
par(mfrow = c(2,4))
ROC_eval("stacked", pred_sl$pred, Y_holdout)
ROC_eval("glm",     pred_sl$library.predict[,1], Y_holdout)
ROC_eval("gam",     pred_sl$library.predict[,2], Y_holdout)
ROC_eval("lasso",   pred_sl$library.predict[,3], Y_holdout)
ROC_eval("ridge",   pred_sl$library.predict[,4], Y_holdout)
ROC_eval("ranger",  pred_sl$library.predict[,5], Y_holdout)
ROC_eval("ksvm",    pred_sl$library.predict[,6], Y_holdout)
ROC_eval("mean",    pred_sl$library.predict[,7], Y_holdout)
par(mfrow = c(1,1))
```

####Create Learners
```{r, StackedRevised1}
SL.ranger
```

```{r, StackedRevised1}
SL.ranger
mtry_seq    <- seq(2, 5, 1)
n.trees_seq <- seq(500, 800, 100) 
learners.ranger <- create.Learner("SL.ranger", tune = list(mtry = mtry_seq, num.trees = n.trees_seq))
learners.ranger$names
set.seed(904)
cv_stacked.ranger <- CV.SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = learners.ranger$names, verbose = F, cvControl = list(V = 5), method = "method.NNLS")
plot(cv_stacked.ranger) + theme_bw()
```

```{r, StackedRevised2}
SL.ranger_5
SL.ranger_13
SL.ranger_14
```

```{r, StackedRevised2, include = F}
SL.glmnet
alpha_seq <- seq(0, 1, 0.1)
learners.glmnet <- create.Learner("SL.glmnet", tune = list(alpha = alpha_seq))
learners.glmnet$names
set.seed(904)
cv_stacked.glmnet <- CV.SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = learners.glmnet$names,
                                     verbose = F, cvControl = list(V = 5), method = "method.NNLS")
review_weights(cv_stacked.glmnet)
summary(cv_stacked.glmnet)
plot(cv_stacked.glmnet) + theme_bw()

SL.gam
deg_seq <- seq(1, 8, 1)
cts_seq <- seq(2, 6, 1)
learners.gam <- create.Learner("SL.gam", tune = list(deg.gam = deg_seq, cts.num = cts_seq))
learners.gam$names
set.seed(904)
cv_stacked.gam <- CV.SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = learners.gam$names,
                                     verbose = T, cvControl = list(V = 5), method = "method.NNLS")
review_weights(cv_stacked.gam)
summary(cv_stacked.gam)
plot(cv_stacked.gam) + theme_bw()
SL.gam_12
```

```{r, StackedRevised3}
SL.gam_12
SL.glmnet_1
SL.ranger_5
SL.library2 <- c("SL.glm", "SL.gam_12", "SL.glmnet_1",  "SL.ranger_5", "SL.mean")
set.seed(904)
cv_stacked2 <- CV.SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = SL.library2,
                               verbose = T, cvControl = list(V = 5), method = "method.NNLS")
review_weights(cv_stacked2)
summary(cv_stacked2)
plot(cv_stacked2) + theme_bw()

sl_stacked2 = SuperLearner(Y = Y_train, X = X_train, family = binomial(), SL.library = SL.library2, 
                          verbose = T, cvControl = list(V = 5), method = "method.NNLS")

pred_sl2 <- predict.SuperLearner(sl_stacked2, newdata = X_holdout, Y = Y_train, X = X_train)
par(mfrow = c(1,2))
ROC_eval("stacked- revised", pred_sl2$pred, Y_holdout)
ROC_eval("gam",              pred_sl$library.predict[,2], Y_holdout)
par(mfrow = c(1,1))
```
